{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "HW1 is due on **11:59 PM PT, Oct 19 (Monday, Week 3)**. Please submit through GradeScope (you will receive an invite to Gradescope for CS145 Fall 2020.). \n",
    "\n",
    "## Print Out Your Name and UID\n",
    "\n",
    "<span style=\"color:blue\"> **Name: Ali Mirabzadeh , UID: 305179067 </span>\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment by the given `cs145hw1.yml` file, which provides the name and necessary packages for this tasks. If you have `conda` properly installed, you may create, activate or deactivate by the following commands:\n",
    "\n",
    "```\n",
    "conda create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "More useful information about managing environments can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You may also quickly review the usage of basic Python and Numpy package, if needed in coding for matrix operations.\n",
    "\n",
    "In this notebook, you must not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you need to highlight these changes. You may add some additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys \n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This workbook will walk you through a linear regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 100)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py` which use the close for solution of $\\hat{\\beta}$.\n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675804\n",
      "Learning Algorithm Type:  0\n",
      "Normalized training error is:  0.08693886675396784\n",
      "Normalized testing error is:  0.11017540281675804\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('0')\n",
    "train_predict = lm.predict(lm.train_x.values, beta)\n",
    "test_predict = lm.predict(lm.test_x.values, beta)\n",
    "training_error =  lm.compute_mse(train_predict,lm.train_y.values)\n",
    "testing_error = lm.compute_mse(test_predict, lm.test_y.values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)\n",
    "\n",
    "## below for normalizing\n",
    "lm.normalize()\n",
    "beta_normalized = lm.train('0')\n",
    "train_predict_n = lm.predict(lm.train_x.values, beta_normalized)\n",
    "test_predict_n = lm.predict(lm.test_x.values, beta_normalized)\n",
    "training_error_n =  lm.compute_mse(train_predict_n,lm.train_y.values)\n",
    "testing_error_n = lm.compute_mse(test_predict_n, lm.test_y.values)\n",
    "print('Normalized training error is: ', training_error_n)\n",
    "print('Normalized testing error is: ', testing_error_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py` which compute the gradient of the objective fuction. \n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training error is:  0.08693919081192665\n",
      "Testing error is:  0.11019432186477264\n",
      "Learning Algorithm Type:  1\n",
      "Normalized training error is:  0.09654351096262996\n",
      "Normalized testing error is:  0.12865993449113894\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('1')\n",
    "train_predict = lm.predict(lm.train_x.values, beta)\n",
    "test_predict = lm.predict(lm.test_x.values, beta)\n",
    "training_error =  lm.compute_mse(train_predict,lm.train_y.values)\n",
    "testing_error = lm.compute_mse(test_predict, lm.test_y.values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)\n",
    "\n",
    "## below for normalizing\n",
    "lm.normalize()\n",
    "beta_normalized = lm.train('1')\n",
    "train_predict_n = lm.predict(lm.train_x.values, beta_normalized)\n",
    "test_predict_n = lm.predict(lm.test_x.values, beta_normalized)\n",
    "training_error_n =  lm.compute_mse(train_predict_n,lm.train_y.values)\n",
    "testing_error_n = lm.compute_mse(test_predict_n, lm.test_y.values)\n",
    "print('Normalized training error is: ', training_error_n)\n",
    "print('Normalized testing error is: ', testing_error_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which use an estimated gradient of the objective function.\n",
    "\n",
    "Train you model by using `lm.train('2')` function.\n",
    "\n",
    "Print the training error and the testing error using `lm.predict` and `lm.compute_mse` given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  2\n",
      "Training error is:  91.10508480511325\n",
      "Testing error is:  96.50500424208748\n",
      "Learning Algorithm Type:  2\n",
      "Normalized training error is:  31.219733063437648\n",
      "Normalized testing error is:  29.420724030847268\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('2')\n",
    "train_predict = lm.predict(lm.train_x.values, beta)\n",
    "test_predict = lm.predict(lm.test_x.values, beta)\n",
    "training_error =  lm.compute_mse(train_predict,lm.train_y.values)\n",
    "testing_error = lm.compute_mse(test_predict, lm.test_y.values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)\n",
    "\n",
    "## below for normalizing\n",
    "lm.normalize()\n",
    "beta_normalized = lm.train('2')\n",
    "train_predict_n = lm.predict(lm.train_x.values, beta_normalized)\n",
    "test_predict_n = lm.predict(lm.test_x.values, beta_normalized)\n",
    "training_error_n =  lm.compute_mse(train_predict_n,lm.train_y.values)\n",
    "testing_error_n = lm.compute_mse(test_predict_n, lm.test_y.values)\n",
    "print('Normalized training error is: ', training_error_n)\n",
    "print('Normalized testing error is: ', testing_error_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the MSE on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Apply z-score normalization for eachh featrure and comment whether or not it affect the three algorithm. \n",
    "3. Ridge regression is adding an L2 regularization term to the original objective function of mean squared error. The objective function become following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} \\sum_i \\left(x_i^T\\beta - y_i \\right)^2 + \\frac{\\lambda}{2n} \\sum_j \\beta_j^2 ,$$ \n",
    "where $\\lambda \\leq 0$, which is a hyper parameter that controls the trade off. Take the derivative of this provided objective function and derive the closed form solution for $\\beta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 1. With some small precision difference, they are the same. We see the error rates to vary because thse are different models. The stochastic gradient descent has the largest error rate because our dataset is not large enough for this method to be efficent. More precise methods like batch gradient descent and linear regression are more effective. </span><br>\n",
    "  <span style=\"color:blue\">   2. You can see each section (1.1, 1.2, 1.3) as I have written the code for it and see the printed result. With the normalized data we can see an increase in error rate for batch gradient descent, no change in error rate for linear regression and a decrease in error rate for stochastic gradient descent. </span> <br>\n",
    "   <span style=\"color:blue\">  3.  $$ \\beta =  \\left(x^T x + Î»\\right)^-1 + \\left(x^T y\\right)$$  </span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This workbook will walk you through a logistic regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 5)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which compute the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check. \n",
    "\n",
    "Train you model by using `lm.train('0')` function.\n",
    "\n",
    "And print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.5561796575778383 \t\n",
      "average logL for iteration 1000: -0.4615656578257467 \t\n",
      "average logL for iteration 2000: -0.4615656578257467 \t\n",
      "average logL for iteration 3000: -0.4615656578257467 \t\n",
      "average logL for iteration 4000: -0.4615656578257467 \t\n",
      "average logL for iteration 5000: -0.4615656578257467 \t\n",
      "average logL for iteration 6000: -0.4615656578257467 \t\n",
      "average logL for iteration 7000: -0.4615656578257467 \t\n",
      "average logL for iteration 8000: -0.4615656578257467 \t\n",
      "average logL for iteration 9000: -0.4615656578257467 \t\n",
      "Training avgLogL:  -0.4615656578257467\n",
      "Training accuracy is:  0.798\n",
      "Testing accuracy is:  0.7495029821073559\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('0')\n",
    "train_predict = lm.predict(lm.train_x.values, beta)\n",
    "test_predict = lm.predict(lm.test_x.values, beta)\n",
    "training_accuracy =  lm.compute_accuracy(train_predict,lm.train_y.values)\n",
    "testing_accuracy = lm.compute_accuracy(test_predict, lm.test_y.values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` in `logistic_regression.py`, which make use of both first and second derivative.\n",
    "\n",
    "Train you model by using `lm.train('1')` function.\n",
    "\n",
    "Print the training and testing accuracy using `lm.predict` and `lm.compute_accuracy` given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -0.7139731229909018 \t\n",
      "average logL for iteration 500: -0.5558529778896625 \t\n",
      "average logL for iteration 1000: -0.49990618171705625 \t\n",
      "average logL for iteration 1500: -0.4766183024021117 \t\n",
      "average logL for iteration 2000: -0.46678224234711174 \t\n",
      "average logL for iteration 2500: -0.4627302012028891 \t\n",
      "average logL for iteration 3000: -0.4611124973709076 \t\n",
      "average logL for iteration 3500: -0.46048365917256584 \t\n",
      "average logL for iteration 4000: -0.46024395049229505 \t\n",
      "average logL for iteration 4500: -0.4601537801568773 \t\n",
      "average logL for iteration 5000: -0.4601201520274992 \t\n",
      "average logL for iteration 5500: -0.46010767881536613 \t\n",
      "average logL for iteration 6000: -0.4601030679263344 \t\n",
      "average logL for iteration 6500: -0.46010136699582815 \t\n",
      "average logL for iteration 7000: -0.46010074033149045 \t\n",
      "average logL for iteration 7500: -0.46010050963239185 \t\n",
      "average logL for iteration 8000: -0.4601004247433812 \t\n",
      "average logL for iteration 8500: -0.4601003935162356 \t\n",
      "average logL for iteration 9000: -0.460100382031068 \t\n",
      "average logL for iteration 9500: -0.46010037780733604 \t\n",
      "Training avgLogL:  -0.4601003762559442\n",
      "Training accuracy is:  0.797\n",
      "Testing accuracy is:  0.7534791252485089\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "train_predict = lm.predict(lm.train_x.values, beta)\n",
    "test_predict = lm.predict(lm.test_x.values, beta)\n",
    "training_accuracy =  lm.compute_accuracy(train_predict,lm.train_y.values)\n",
    "testing_accuracy = lm.compute_accuracy(test_predict, lm.test_y.values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?\n",
    "2. Regularization. Similar to linear regression, an regularization term could be added to logistic regression. \n",
    "The objective function becomes following: \n",
    "    $$ J(\\beta) = -\\frac{1}{n} \\sum_i \\left(y_i x_i^T \\beta - \\log \\left( 1+ \\exp\\{ x_i^T \\beta \\} \\right) \\right) + \\lambda \\sum_j \\beta_j^2,$$ \n",
    "where $\\lambda \\leq 0$, which is a hyper parameter that controls the trade off. Take the derivative $\\frac{\\partial J(\\beta)}{\\partial \\beta_j}$ of this provided objective function and provide the batch gradient descent update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 1. They are the same with a very small difference in their precisions. And it makes sense as they both have the same optimal point;both algorithms are guarenteeed to converge. </span><br>\n",
    "<span style=\"color:blue\"> 2. $$ \\beta_{new} = \\beta_{old} + \\frac{\\partial J(\\beta)}{\\partial \\beta_j} $$ \n",
    "    $$ \\frac{\\partial J(\\beta)}{\\partial \\beta_j} = (XY - X \\sigma(\\beta) + 2 \\lambda \\beta) $$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the same implementation for another small dataset with each datapoint $x$ with only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (99, 2)\n",
      "Training labels shape: (99,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training avgLogL:  -0.3291474312957121\n",
      "Training accuracy is:  0.8888888888888888\n",
      "Beta:  [-0.04717577  1.46005896  2.06586134]\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "train_predict = lm.predict(lm.train_x.values, beta)\n",
    "training_accuracy =  lm.compute_accuracy(train_predict,lm.train_y.values)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Beta: ', beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Yes, because the probablity can be written as a linear function p = beta.dot(x) which is linear as we are classifying the two features and the line seperates them</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "    \n",
    "# plot the decision boundary on top of the scattered points\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "db = -(beta[0] + beta[1] * df[['x1','x2']]) / beta[2]\n",
    "plt.plot(X, db, 'r^')\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1 :)\n",
    "After you've finished the homework, please print out the entire `ipynb` notebook and two `py` files into one PDF file. Make sure you include the output of code cells and answers for questions. Prepare submit it to GradeScope. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
